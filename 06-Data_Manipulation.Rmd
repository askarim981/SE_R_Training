# Data manipulation

This chapter will focus on the fundamentals of data manipulation. You can identify what the key functions do from the [dplyr cheat cheat](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf): 
As the sheet states, the best way to manipulate data is if your data is tidy (wide) - i.e. each variable is in its own column, and each observation is in its own row. Occasionally you will need long data, such as to build graphs.

This sheet demonstrates common data manipulation techniques using the dplyr package in R. Examples use generic placeholder data that can be adapted to your specific datasets.

## Setup and Data Loading

First, we'll load the required packages and read in our data. The `dplyr` package is part of the tidyverse and provides powerful data manipulation functions.

```{r setup, message=FALSE, warning=FALSE}
# Load required packages
library(dplyr)
library(readr)
library(tidyr)
library(janitor)
library(stringr)
```

Next, we'll read in our main dataset and any supplementary lookup data. This example shows how to handle common data cleaning tasks during the import process.

```{r load-data, eval=FALSE}
# Read in your main dataset
main_data <- read.csv("path/to/your/main_data.csv", skip = 0, check.names = FALSE)

# Read in supplementary lookup data
lookup_data <- read.csv("path/to/your/lookup_data.csv")
```

Often, you'll need to perform basic cleaning and joining operations immediately after loading your data. This code demonstrates how to convert character columns to numeric, join datasets, and filter out unwanted rows.

```{r clean-data, eval=FALSE}
# Basic data cleaning and joining
clean_data <- main_data %>%
  mutate_at(vars(numeric_columns), readr::parse_number) %>%  # Convert character columns to numeric
  left_join(lookup_data %>% select(id_column, category_column), by = "id_column") %>%
  filter(entity_column != "Total")  # Remove summary rows
```

## Selecting Columns

The `select()` function is your primary tool for choosing which columns to keep in your dataset. It's much more powerful than simple column indexing and provides several flexible approaches.

### Basic Column Selection

The most straightforward approach is to select columns by name. This is the most reliable method since column names are less likely to change than positions.

```{r select-basic, eval=FALSE}
# Select specific columns by name
subset_data <- clean_data %>%
  select(id_column, name_column)

# Using a predefined vector for column names
desired_columns <- c("id_column", "name_column", "value_column")
subset_data2 <- clean_data %>%
  select(all_of(desired_columns))
```

### Excluding Columns

Sometimes it's easier to specify what you don't want rather than what you do want, especially when you have many columns.

```{r select-exclude, eval=FALSE}
# Select all columns except specific ones
no_id_data <- clean_data %>%
  select(-id_column)

# Exclude multiple columns
no_admin_data <- clean_data %>%
  select(-c(id_column, created_date, updated_date))
```

### Advanced Selection Techniques

You can select columns by position (though this should be used cautiously), by data type, or by pattern matching.

```{r select-advanced, eval=FALSE}
# Select columns by position (use cautiously)
first_columns <- clean_data %>%
  select(1:5)

# Select columns by data type
numeric_columns <- clean_data %>%
  select(where(is.numeric))

# Select columns by pattern
value_columns <- clean_data %>%
  select(starts_with("value_"))

# Select and rename columns simultaneously
renamed_data <- clean_data %>%
  select(identifier = id_column, entity = name_column, category = category_column)
```

## Filtering Rows

The `filter()` function allows you to subset your data based on logical conditions. This is essential for focusing your analysis on specific subsets of your data.

### Setting Up Custom Operators

Before diving into filtering examples, it's useful to define some custom operators that make filtering more intuitive.

```{r filter-setup, eval=FALSE}
# Define custom operators
`%notin%` <- Negate(`%in%`)
```

### Basic Filtering Operations

These examples cover the most common filtering scenarios you'll encounter in data analysis.

```{r filter-basic, eval=FALSE}
# Exact match filtering
filtered_data <- clean_data %>%
  filter(id_column == 12345)

# Text matching
filtered_data2 <- clean_data %>%
  filter(name_column == "Specific Entity")

# Multiple value matching
filtered_data3 <- clean_data %>%
  filter(name_column %in% c("Entity A", "Entity B", "Entity C"))

# Pattern matching with regular expressions
filtered_data4 <- clean_data %>%
  filter(grepl('keyword', name_column))  # Contains 'keyword'

# Negative pattern matching
filtered_data5 <- clean_data %>%
  filter(!grepl('keyword', name_column))  # Does NOT contain 'keyword'
```

### Logical Operators in Filtering

You can combine multiple conditions using logical operators to create more complex filters.

```{r filter-logical, eval=FALSE}
# OR condition - either condition can be true
filtered_data6 <- clean_data %>%
  filter(name_column == "Entity A" | name_column == "Entity B")

# AND condition - both conditions must be true
filtered_data7 <- clean_data %>%
  filter(grepl('keyword1', name_column) & grepl('keyword2', name_column))

# Handle missing values
filtered_data8 <- clean_data %>%
  filter(!is.na(category_column))

# Multiple conditions in one filter (implicit AND)
filtered_data9 <- clean_data %>%
  filter(value_column1 == 0,
         value_column2 > 100,
         !is.na(category_column))
```

### Advanced Filtering with across()

The `across()` function allows you to apply the same filtering logic to multiple columns simultaneously.

```{r filter-advanced, eval=FALSE}
# Filter rows where specified columns contain specific patterns
filtered_advanced <- clean_data %>%
  filter(across(c(column1, column2), ~ str_detect(., pattern = "pattern")))

# Filter rows with no letters in numeric columns
filtered_numeric <- clean_data %>%
  filter(across(c(numeric_col1, numeric_col2), ~ !str_detect(., pattern = "[A-Z]")))

# Filter rows where values are above threshold
filtered_threshold <- clean_data %>%
  filter_at(vars(value_col1, value_col2), all_vars(. > 100))

# Filter out rows with any missing values
complete_cases <- clean_data %>%
  filter_at(vars(everything()), all_vars(!is.na(.)))
```

## Summarising Data

The `summarise()` function creates summary statistics from your data. It's particularly powerful when combined with grouping operations.

### Basic Summarisation

Here's how to create summary statistics for your entire dataset:

```{r summarise-basic, eval=FALSE}
# Basic summary of entire dataset
total_summary <- clean_data %>%
  summarise(
    total_count = n(),
    total_value = sum(value_column, na.rm = TRUE),
    average_value = mean(value_column, na.rm = TRUE),
    max_value = max(value_column, na.rm = TRUE),
    min_value = min(value_column, na.rm = TRUE),
    std_dev = sd(value_column, na.rm = TRUE)
  )

# View the results
print(total_summary)
```

### Summarising Multiple Columns

When you need to apply the same summary functions to multiple columns, use `across()` to avoid repetitive code:

```{r summarise-multiple, eval=FALSE}
# Summarise multiple columns at once
multi_summary <- clean_data %>%
  summarise(across(c(value_col1, value_col2), 
                   list(sum = ~ sum(.x, na.rm = TRUE),
                        mean = ~ mean(.x, na.rm = TRUE),
                        count = ~ sum(!is.na(.x))), 
                   .names = "{.col}_{.fn}"))

# This creates columns like value_col1_sum, value_col1_mean, value_col1_count, etc.
```

## Grouping and Group-wise Operations

Grouping is one of the most powerful features of dplyr. It allows you to perform operations on subsets of your data defined by the values in one or more columns.

### Single Variable Grouping

The most common use case is grouping by a single categorical variable:

```{r group-single, eval=FALSE}
# Group by a single variable
grouped_summary <- clean_data %>%
  group_by(category_column) %>%
  summarise(
    count = n(),
    total_value = sum(value_column, na.rm = TRUE),
    average_value = mean(value_column, na.rm = TRUE),
    median_value = median(value_column, na.rm = TRUE),
    .groups = "drop"  # This removes the grouping after summarising
  )

# View the results
print(grouped_summary)
```

### Multiple Variable Grouping

You can group by multiple variables to create more detailed breakdowns:

```{r group-multiple, eval=FALSE}
# Create a new categorical variable first
analysis_data <- clean_data %>%
  mutate(type_flag = if_else(grepl('keyword', name_column), "Type A", "Type B"))

# Group by multiple variables
multi_grouped <- analysis_data %>%
  group_by(category_column, type_flag) %>%
  summarise(
    count = n(),
    total_value = sum(value_column, na.rm = TRUE),
    average_value = mean(value_column, na.rm = TRUE),
    .groups = "drop"
  )

# This creates a row for each unique combination of category_column and type_flag
```

### Simple Counting

Sometimes you just need to count how many observations fall into each group:

```{r group-count, eval=FALSE}
# Count occurrences by group
count_by_group <- clean_data %>%
  count(category_column, sort = TRUE)  # sort = TRUE orders by count descending

# Alternative using group_by and summarise
count_by_group2 <- clean_data %>%
  group_by(category_column) %>%
  summarise(row_count = n(), .groups = "drop")
```

## Adding Totals

The `janitor` package provides convenient functions for adding total rows and columns to your summaries.

```{r totals, eval=FALSE}
# Add row totals (adds a "Total" row at the bottom)
data_with_totals <- grouped_summary %>%
  adorn_totals("row")

# Add column totals (adds a "Total" column on the right)
data_with_col_totals <- grouped_summary %>%
  adorn_totals("col")

# Add both row and column totals
data_with_both_totals <- grouped_summary %>%
  adorn_totals(c("row", "col"))
```

You can verify that your totals are correct and extract specific values:

```{r verify-totals, eval=FALSE}
# Verify totals match your expectations
total_check <- data_with_totals$total_value[nrow(data_with_totals)]

# Extract specific total value
extracted_total <- data_with_totals %>%
  slice_tail(n = 1) %>%
  pull(total_value)

# Check if totals match between different summaries
print(paste("Total matches:", total_check == sum(clean_data$value_column, na.rm = TRUE)))
```

## Creating and Transforming Variables

The `mutate()` function is your primary tool for creating new variables or transforming existing ones. It's incredibly flexible and supports complex operations.

### Basic Variable Creation

These examples show the most common ways to create new variables:

```{r mutate-basic, eval=FALSE}
# Create new variables
transformed_data <- clean_data %>%
  mutate(
    # Simple arithmetic
    combined_value = value_col1 + value_col2 + value_col3,
    
    # Percentage calculations
    percentage = (value_column / total_column) * 100,
    
    # Simple conditional variable
    binary_flag = if_else(value_column > 100, "High", "Low"),
    
    # Overwrite existing column (apply a transformation)
    value_column = value_column * 1.1  # Apply 10% increase
  )
```

### Complex Conditional Logic

For more complex conditions, use `case_when()`, which is like a vectorised if-else statement:

```{r mutate-complex, eval=FALSE}
# Complex conditional variable with multiple outcomes
complex_data <- clean_data %>%
  mutate(
    category_flag = case_when(
      grepl("pattern1", name_column) ~ "Category 1",
      grepl("pattern2", name_column) ~ "Category 2",
      value_column > 500 ~ "High Value",
      value_column < 50 ~ "Low Value",
      is.na(value_column) ~ "Missing",
      TRUE ~ "Other"  # This handles all remaining cases
    ),
    
    # Combining multiple conditions
    complex_flag = case_when(
      grepl("important", name_column) & value_column > 100 ~ "Important and High",
      grepl("important", name_column) & value_column <= 100 ~ "Important but Low",
      !grepl("important", name_column) & value_column > 100 ~ "Not Important but High",
      TRUE ~ "Other"
    )
  )
```

### Transforming Multiple Columns

When you need to apply the same transformation to multiple columns, use `across()` or `mutate_at()`:

```{r mutate-multiple, eval=FALSE}
# Transform multiple columns simultaneously
multi_transformed <- clean_data %>%
  mutate(across(c(value_col1, value_col2), ~ .x * 1000))  # Convert to thousands

# Apply custom function to multiple columns
custom_function <- function(x) {
  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)  # Standardise
}

standardised_data <- clean_data %>%
  mutate(across(starts_with("value_"), custom_function))

# Transform all numeric columns
all_numeric_transformed <- clean_data %>%
  mutate(across(where(is.numeric), ~ round(.x, 2)))  # Round to 2 decimal places
```

### Row-wise Operations

Sometimes you need to perform calculations across columns within each row:

```{r mutate-rowwise, eval=FALSE}
# Create columns from row-wise operations
row_operations <- clean_data %>%
  mutate(
    # Sum across multiple columns
    row_sum = rowSums(select(., starts_with("value_")), na.rm = TRUE),
    
    # Average across multiple columns
    row_mean = rowMeans(select(., starts_with("value_")), na.rm = TRUE),
    
    # Count non-missing values per row
    non_missing_count = rowSums(!is.na(select(., starts_with("value_")))),
    
    # Find maximum value per row
    row_max = do.call(pmax, c(select(., starts_with("value_")), na.rm = TRUE))
  )
```

### Handling Missing Values

Missing values are common in real data. Here's how to handle them systematically:

```{r mutate-missing, eval=FALSE}
# Replace missing values with different strategies
clean_nas <- clean_data %>%
  mutate(
    # Replace with zero
    value_col1 = replace_na(value_col1, 0),
    
    # Replace with mean
    value_col2 = replace_na(value_col2, mean(value_col2, na.rm = TRUE)),
    
    # Replace all numeric columns with zero
    across(where(is.numeric), ~replace_na(., 0)),
    
    # Replace text columns with "Unknown"
    across(where(is.character), ~replace_na(., "Unknown"))
  )
```

## Pivoting Data (Reshaping)

Data can be organised in "wide" format (variables spread across columns) or "long" format (variables stacked in rows). The `pivot_longer()` and `pivot_wider()` functions help you convert between these formats.

### Wide to Long Format

Long format is often better for analysis and visualisation:

```{r pivot-long, eval=FALSE}
# Convert wide data to long format
long_data <- clean_data %>%
  # First, select only the columns you want to pivot
  select(id_column, name_column, starts_with("value_")) %>%
  
  # Then pivot the value columns into long format
  pivot_longer(
    cols = starts_with("value_"),      # Columns to pivot
    names_to = "metric",               # Name for the new column containing old column names
    values_to = "value",               # Name for the new column containing values
    names_prefix = "value_"            # Remove this prefix from the metric names
  )

# View the structure
head(long_data)
```

### Long to Wide Format

Sometimes you need to convert long data back to wide format:

```{r pivot-wide, eval=FALSE}
# Convert long data back to wide format
wide_data <- long_data %>%
  pivot_wider(
    names_from = metric,               # Column containing the new column names
    values_from = value,               # Column containing the values
    values_fill = 0,                   # Fill missing combinations with 0
    names_prefix = "value_"            # Add this prefix to the new column names
  )
```

### Advanced Pivoting

You can also pivot multiple columns simultaneously:

```{r pivot-advanced, eval=FALSE}
# Pivot multiple value columns
complex_long <- clean_data %>%
  pivot_longer(
    cols = c(starts_with("value_"), starts_with("count_")),
    names_to = c("metric", "type"),
    names_pattern = "(.+)_(.+)",       # Extract parts of column names
    values_to = "amount"
  )
```

## Joining Datasets

Joining datasets is crucial for combining information from multiple sources. The type of join you choose depends on how you want to handle records that don't have matches.

### Basic Join Operations

The most common join is a left join, which keeps all records from the left dataset:

```{r join-basic, eval=FALSE}
# Basic left join - keeps all records from main_data
joined_data <- main_data %>%
  left_join(lookup_data, by = "common_column")

# Inner join - keeps only records that exist in both datasets
inner_joined <- main_data %>%
  inner_join(lookup_data, by = "common_column")

# Full join - keeps all records from both datasets
full_joined <- main_data %>%
  full_join(lookup_data, by = "common_column")
```

### Handling Different Column Names

Often, the columns you want to join on have different names in each dataset:

```{r join-different-names, eval=FALSE}
# Join when column names don't match
joined_different_names <- main_data %>%
  left_join(lookup_data, by = c("main_id" = "lookup_id"))

# You can also rename before joining
joined_renamed <- main_data %>%
  left_join(lookup_data %>% rename(main_id = lookup_id), by = "main_id")
```

### Multiple Column Joins

Sometimes you need to join on multiple columns to create a unique match:

```{r join-multiple, eval=FALSE}
# Join on multiple columns
multi_column_join <- main_data %>%
  left_join(lookup_data, by = c("column1", "column2", "column3"))

# Mix of same and different column names
mixed_join <- main_data %>%
  left_join(lookup_data, by = c("same_name", "main_col" = "lookup_col"))
```

### Joining Multiple Datasets

When you have several datasets to join, you can use `reduce()` to join them all at once:

```{r join-multiple-datasets, eval=FALSE}
# Join multiple datasets with a common structure
dataset_list <- list(
  data1 = data1,
  data2 = data2,
  data3 = data3,
  data4 = data4
)

# Join all datasets on common columns
multiple_joined <- dataset_list %>%
  reduce(left_join, by = c('id_column', 'name_column'))
```

## Practical Examples and Patterns

### Complete Data Processing Pipeline

Here's a comprehensive example that combines many of the techniques we've covered:

```{r complete-pipeline, eval=FALSE}
# Complete data processing pipeline
processed_data <- raw_data %>%
  # Step 1: Clean and standardise the data
  mutate_at(vars(starts_with("numeric_")), parse_number) %>%
  filter(!is.na(id_column), !grepl("test", name_column, ignore.case = TRUE)) %>%
  
  # Step 2: Join supplementary data
  left_join(lookup_data, by = "id_column") %>%
  
  # Step 3: Create derived variables
  mutate(
    # Calculate totals
    total_score = rowSums(select(., starts_with("score_")), na.rm = TRUE),
    
    # Create categories
    performance_category = case_when(
      total_score >= 80 ~ "High",
      total_score >= 60 ~ "Medium",
      total_score >= 40 ~ "Low",
      TRUE ~ "Very Low"
    ),
    
    # Create flags
    has_complete_data = !is.na(total_score) & !is.na(category_column),
    
    # Calculate percentages
    score_percentage = (total_score / max(total_score, na.rm = TRUE)) * 100
  ) %>%
  
  # Step 4: Filter to final analysis sample
  filter(has_complete_data, total_score > 0) %>%
  
  # Step 5: Group and summarise
  group_by(performance_category, region) %>%
  summarise(
    count = n(),
    avg_score = mean(total_score, na.rm = TRUE),
    median_score = median(total_score, na.rm = TRUE),
    std_dev = sd(total_score, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  
  # Step 6: Add totals and format
  adorn_totals("row") %>%
  mutate(across(where(is.numeric), ~ round(.x, 2)))
```

### Function-Based Processing

For repetitive tasks, create custom functions:

```{r custom-functions, eval=FALSE}
# Create a function to standardise dataset processing
process_dataset <- function(df, dataset_name) {
  df %>%
    # Add source identifier
    mutate(source = dataset_name) %>%
    
    # Standardise column names
    select(id = id_column, 
           name = name_column,
           value = value_column, 
           category = category_column,
           source) %>%
    
    # Clean data
    filter(!is.na(value), value > 0) %>%
    
    # Add standardised metrics
    mutate(
      value_scaled = scale(value)[,1],
      value_log = log(value + 1)
    )
}

# Apply function to multiple datasets
datasets <- list(
  africa = africa_data,
  asia = asia_data,
  europe = europe_data,
  americas = americas_data
)

# Process all datasets and combine
combined_data <- map_dfr(datasets, process_dataset, .id = "region")
```

## Tips and Best Practices

### Data Validation

Always validate your data transformations:

```{r validation, eval=FALSE}
# Check data before and after transformations
original_rows <- nrow(raw_data)
processed_rows <- nrow(processed_data)

print(paste("Original rows:", original_rows))
print(paste("Processed rows:", processed_rows))
print(paste("Rows lost:", original_rows - processed_rows))

# Check for unexpected missing values
missing_summary <- processed_data %>%
  summarise(across(everything(), ~ sum(is.na(.))))

print("Missing values by column:")
print(missing_summary)
```

### Performance Considerations

For large datasets, consider these optimisation strategies:

```{r performance-tips, eval=FALSE}
# For large datasets, filter early to reduce processing time
large_data_processed <- large_dataset %>%
  filter(important_flag == TRUE) %>%  # Filter first
  mutate(expensive_calculation = complex_function(value_column)) %>%  # Then transform
  group_by(category) %>%
  summarise(result = mean(expensive_calculation))

# Use data.table for very large datasets
library(data.table)
setDT(large_dataset)
result <- large_dataset[important_flag == TRUE, .(mean_value = mean(value_column)), by = category]
```

### Debugging and Troubleshooting

When your code doesn't work as expected:

```{r debugging, eval=FALSE}
# Test transformations on small subsets first
test_data <- clean_data %>%
  slice_head(n = 10)

# Check intermediate results
debug_pipeline <- clean_data %>%
  mutate(new_variable = some_transformation) %>%
  {print(paste("Rows after mutate:", nrow(.))); .} %>%  # Debug print
  filter(some_condition) %>%
  {print(paste("Rows after filter:", nrow(.))); .} %>%  # Debug print
  group_by(category) %>%
  summarise(result = mean(new_variable))
```


In order to make a data frame that's easy to subset, you might want to define the row names. When defining row names, each one needs to be unique. I.e. you should use wide data, and the row names should be a unique column. 

For this example, we'll create our own data frame. However, if you want to see a dataset where a column has already been made into row names, see the mtcars data. A pre-loaded dataset in R. (run`data("mtcars)`) 


```{r eval=FALSE}
# Define a 5x3 data frame
Test_Data <- as.data.frame(tibble(name = c("Abdus", "Sarah", "Yilan", "Michael", "Alex"),
                    office = c("CH", "SB", "PG", "PG", "CH"),
                    manager = c("Sarah", "Yilan", "Frank", "Yilan", "Michael")))

#It looks like this:
#   name   office manager
#     name office manager
# 1   Abdus     CH   Sarah
# 2   Sarah     SB   Yilan
# 3   Yilan     PG   Frank
# 4 Michael     PG   Yilan
# 5    Alex     CH Michael

# You'll see our row names are 1:5, therefore you can extract the first row in the manager column like this:

Abdus_Manager1 <- Test_Data["1", "manager"]
print(Abdus_Manager1)
# [1] "Sarah"

# NOTE: if the Test_Data was not a data frame (i.e. was just a tibble). You would need to do pull(Test_Data["1", "manager"]) OR Test_Data["1", "manager"] %>% pull()

# We can give the table row names using the column to row names
# This wouldn't work for the `office` column because there are multiple PGs

Col_Indexed_Test_Data <- Test_Data %>%
  column_to_rownames(., var = "name")

# Our data now look like this
#        office manager
#        office manager
#Abdus       CH   Sarah
#Sarah       SB   Yilan
#Yilan       PG   Frank
#Michael     PG   Yilan
#Alex        CH Michael


#Now we can pull with specified inputs. 
Abdus_Manager2 <- Col_Indexed_Test_Data["Abdus", "manager"]
print(Abdus_Manager2)
#[1] "Sarah"
```

An alternative way to extract data is by filtering columns until you end up with a single row, at which point you can use the `pull()` function to extract the data point in the column you are after.


```{r eval=FALSE}
Abdus_Manager3 <- Test_Data %>%
  filter(name == "Abdus") %>%
  pull(manager)
```

Use `[[]]` to return an element in a list; use for [recurssive indexing](https://www.geeksforgeeks.org/difference-between-single-and-double-square-brackets-in-r/). See `iris[[2]]` vs `iris[2]`. Double square brackets are often necessary to pinpoint elements. 


## Others

Most data manipulation functions can be found in the dplyr cheat sheet and are fairly intuitive to use. 
Particularly useful are binding functions: `cbind()` in `BaseR`, `bind_cols()` in `dplyr`. Also `pull()` and `rename()` 

## Quiz: Data Manipulation

```{r DM Quiz, echo=FALSE}
# Function to create a quiz on data manipulation with dplyr
create_manipulation_quiz <- function() {
  # A list of questions, options, and answers
  questions <- list(
    list(
      q = "Which dplyr function is used to select columns from a data frame?",
      options = c("A) filter()", "B) select()", "C) mutate()", "D) arrange()"),
      answer = "B"
    ),
    list(
      q = "To keep rows where `category_column` is either 'A', 'B', or 'C', which filter condition is most suitable?",
      options = c(
        "A) filter(category_column | c('A', 'B', 'C'))",
        "B) filter(category_column %in% c('A', 'B', 'C'))",
        "C) filter(category_column == c('A', 'B', 'C'))",
        "D) filter(grepl('A|B|C', category_column))"
      ),
      answer = "B"
    ),
    list(
      q = "What is the primary purpose of combining `group_by()` and `summarise()`?",
      options = c(
        "A) To create new columns based on complex conditions.",
        "B) To reshape data from wide to long format.",
        "C) To calculate summary statistics for different groups within the data.",
        "D) To join two different data frames together."
      ),
      answer = "C"
    ),
    list(
      q = "Which function is best for creating a new column using complex, multi-level conditional logic?",
      options = c("A) if_else()", "B) select()", "C) case_when()", "D) filter()"),
      answer = "C"
    ),
    list(
      q = "To reshape a 'wide' data frame with many columns (e.g., `value_2020`, `value_2021`) into a 'long' format, which function should you use?",
      options = c("A) pivot_wider()", "B) gather()", "C) pivot_longer()", "D) spread()"),
      answer = "C"
    ),
    list(
      q = "In a `left_join(x, y, by = 'id')`, what happens to the rows in `x` that do not have a matching `id` in `y`?",
      options = c(
        "A) They are removed from the final result.",
        "B) They are kept, and the new columns from y are filled with NA.",
        "C) The join operation produces an error.",
        "D) They are kept, and the new columns from y are filled with 0."
      ),
      answer = "B"
    ),
    list(
      q = "Which function from the `janitor` package is used to add a 'Total' row to a summary table?",
      options = c("A) add_totals()", "B) adorn_totals()", "C) make_totals()", "D) summarise_totals()"),
      answer = "B"
    )
  )

  # Loop through and print each question and its options
  for (i in seq_along(questions)) {
    cat("**", i, ". ", questions[[i]]$q, "**\n\n", sep = "")
    for (option in questions[[i]]$options) {
      cat(option, "\n")
    }
    cat("\n")
  }

  # Print the answers section
  cat("---\n\n")
  cat("**Answers:**\n\n")

  # Loop through and print each correct answer
  for (i in seq_along(questions)) {
    cat(i, ". ", questions[[i]]$answer, "\n", sep = "")
  }
}

# Run the function to display the quiz
create_manipulation_quiz()
```





